{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-11T00:37:28.768636Z",
     "start_time": "2025-04-11T00:37:28.752115Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from functions import shift_atoms, transform_list_hkl_p63, get_structure_factor\n",
    "\n",
    "\n",
    "# Get the number of logical CPU cores available\n",
    "num_threads = 8  # Automatically use all available CPU cores\n",
    "\n",
    "# Configure TensorFlow to use multiple threads\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_threads)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_threads)\n",
    "\n",
    "print(f\"Using {num_threads} threads for intra-op and inter-op parallelism.\")\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 threads for intra-op and inter-op parallelism.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T00:37:44.502711Z",
     "start_time": "2025-04-11T00:37:44.488813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fun_tf(x, pars):\n",
    "    \"\"\"\n",
    "    Function depending on parameters to be fitted.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor\n",
    "        3D Tensor of shape [N, 3] representing the hkl vectors\n",
    "    pars : List or Tensor\n",
    "        1D Tensor with six elements representing the parameters\n",
    "    \"\"\"\n",
    "    modified_struct  = shift_atoms(*pars)\n",
    "    struct_self = tf.map_fn(\n",
    "    lambda hkl: get_structure_factor(hkl, modified_struct), \n",
    "    x, \n",
    "    fn_output_signature=tf.complex64  \n",
    "    )\n",
    "    \n",
    "    # Compute intensity\n",
    "    intensity = tf.abs(struct_self) ** 2\n",
    "    # Normalize the intensity to the maximum value\n",
    "    return intensity / tf.reduce_max(intensity)"
   ],
   "id": "1c3a8c359b693563",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T00:37:45.108081Z",
     "start_time": "2025-04-11T00:37:45.093065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define your data pipeline with tf.data\n",
    "def preprocess_data(features, labels):\n",
    "    # Perform any necessary preprocessing steps here\n",
    "    return features, labels"
   ],
   "id": "15116b1e97bae64d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T00:37:45.573136Z",
     "start_time": "2025-04-11T00:37:45.545375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experimental_data = pd.read_csv('/home/mariolb/repos/CrystalClearFit/DistortionFit/new_data.csv')\n",
    "\n",
    "hkl_list = experimental_data[[\"h\", \"k\", \"l\"]].values.tolist()\n",
    "hkl_list_trans = transform_list_hkl_p63(hkl_list)\n",
    "\n",
    "features = hkl_list_trans\n",
    "labels = tf.convert_to_tensor(experimental_data[\"intensity_exp\"].tolist(), dtype=tf.float32)\n",
    "\n",
    "n_features = experimental_data.shape[0]\n",
    "n_dim = 3\n",
    "\n",
    "max_par_value = 0.1"
   ],
   "id": "ee69ec4cf6516612",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T00:37:46.086370Z",
     "start_time": "2025-04-11T00:37:46.078362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FunAsLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_par_value=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_par_value = max_par_value\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Define six trainable parameters\n",
    "        self.a = self.add_weight(name='a', shape=(), initializer=tf.keras.initializers.he_uniform(), trainable=True)\n",
    "        self.b = self.add_weight(name='b', shape=(), initializer=tf.keras.initializers.he_uniform(), trainable=True)\n",
    "        self.c = self.add_weight(name='c', shape=(), initializer=tf.keras.initializers.he_uniform(), trainable=True)\n",
    "        self.d = self.add_weight(name='d', shape=(), initializer=tf.keras.initializers.he_uniform(), trainable=True)\n",
    "        self.e = self.add_weight(name='e', shape=(), initializer=tf.keras.initializers.he_uniform(), trainable=True)\n",
    "        self.f = self.add_weight(name='f', shape=(), initializer=tf.keras.initializers.he_uniform(), trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply tanh to ensure parameters stay within the [-0.1, 0.1] range\n",
    "        a = self.max_par_value * tf.tanh(self.a)\n",
    "        b = self.max_par_value * tf.tanh(self.b)\n",
    "        c = self.max_par_value * tf.tanh(self.c)\n",
    "        d = self.max_par_value * tf.tanh(self.d)\n",
    "        e = self.max_par_value * tf.tanh(self.e)\n",
    "        f = self.max_par_value * tf.tanh(self.f)\n",
    "        \n",
    "        # Call your fun_tf function to get the output\n",
    "        return fun_tf(inputs, [a, b, c, d, e, f])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # If the input has shape (None, 3), return the same shape\n",
    "        return (input_shape[0], 1)  # This assumes your output is of shape (None, 1)\n"
   ],
   "id": "599a2dea10bd479c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T00:37:46.996509Z",
     "start_time": "2025-04-11T00:37:46.978839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the custom loss function\n",
    "class RFactorLoss(tf.keras.losses.Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        return tf.reduce_sum(tf.abs(y_true - y_pred)) / tf.reduce_sum(tf.abs(y_true))\n",
    "\n",
    "# Define the custom metric function\n",
    "def r_factor_metric(y_true, y_pred):\n",
    "    return tf.reduce_sum(tf.abs(y_true - y_pred)) / tf.reduce_sum(tf.abs(y_true))\n",
    "\n",
    "# Instantiate the Adam optimizer\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-3)\n"
   ],
   "id": "af205339068cb3db",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T00:42:23.280798Z",
     "start_time": "2025-04-11T00:42:22.764419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a TensorFlow Dataset with parallel loading\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "min_loss = 1e10\n",
    "# Optimize data pipeline by parallelizing data loading\n",
    "dataset = dataset.map(preprocess_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.batch(64)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)  # Prefetch to optimize I/O\n",
    "# Create the model\n",
    "inputs = tf.keras.Input(shape=(n_dim,))\n",
    "outputs = FunAsLayer(max_par_value)(inputs)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model with the custom loss function and metric\n",
    "model.compile(\n",
    "    optimizer=optim,\n",
    "    loss=RFactorLoss(),\n",
    "    metrics=[r_factor_metric]\n",
    ")\n",
    "\n",
    "# Store loss values for this iteration\n",
    "iteration_losses = []\n",
    "\n",
    "# Start training loop\n",
    "n_iter = 5  # Example: running for 5 iterations\n",
    "for i in range(n_iter):\n",
    "    histories = []\n",
    "    # Use tqdm to create a custom progress bar for each epoch\n",
    "    with tqdm(total=500, desc=f\"Iteration {i+1}\") as pbar:\n",
    "        for epoch in range(500):\n",
    "            # Train the model for one step (epoch)\n",
    "            history = model.fit(\n",
    "                x=features,  \n",
    "                y=labels,    \n",
    "                batch_size=64,\n",
    "                epochs=1,  # Train for one epoch at a time\n",
    "                verbose=0  # No output during training\n",
    "            )\n",
    "            epoch_loss = history.history['loss'][-1]\n",
    "            histories.append(epoch_loss)\n",
    "            pbar.update(1)  # Update progress bar\n",
    "            pbar.set_postfix(loss=epoch_loss)  # Optionally display loss\n",
    "    \n",
    "    # Store the loss values for this iteration\n",
    "    iteration_losses.append(histories)\n",
    "    # Check final loss\n",
    "    final_loss = iteration_losses[-1]\n",
    "\n",
    "    if final_loss < min_loss:\n",
    "        # Update best model parameters\n",
    "        best_model_pars = [max_par_value * tf.sigmoid(model.layers[-1].get_weights()[i]) for i in range(6)]\n",
    "        min_loss = final_loss\n",
    "        rf = r_factor_metric(labels, fun_tf(features, best_model_pars))\n",
    "        print(f\"Iteration {i+1} - New best loss: {min_loss:.2e} (R-factor: {rf:.2e})\")\n",
    "    "
   ],
   "id": "102164d17a80f1b5",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Optimizer (<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001CB5731E3B0>) passed to `model.compile` was created inside a different distribution strategy scope than the model. All optimizers must be created in the same distribution strategy scope as the model (in this case <tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000001CB5731F250>). If you pass in a string identifier for an optimizer to compile, the optimizer will automatically be created in the correct distribution strategy scope.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mModel(inputs, outputs)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Compile the model with the custom loss function and metric\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRFactorLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mr_factor_metric\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Store loss values for this iteration\u001B[39;00m\n\u001B[0;32m     21\u001B[0m iteration_losses \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:3569\u001B[0m, in \u001B[0;36mModel._validate_compile\u001B[1;34m(self, optimizer, metrics, **kwargs)\u001B[0m\n\u001B[0;32m   3567\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(opt, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_weights\u001B[39m\u001B[38;5;124m\"\u001B[39m, []):\n\u001B[0;32m   3568\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m strategy\u001B[38;5;241m.\u001B[39mextended\u001B[38;5;241m.\u001B[39mvariable_created_in_scope(v):\n\u001B[1;32m-> 3569\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   3570\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00moptimizer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) passed to `model.compile` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3571\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwas created inside a different distribution strategy \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3572\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscope than the model. All optimizers must be created \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3573\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min the same distribution strategy scope as the model \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3574\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(in this case \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstrategy\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m). If you pass in a string \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3575\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124midentifier for an optimizer to compile, the optimizer \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3576\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwill automatically be created in the correct \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3577\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistribution strategy scope.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3578\u001B[0m         )\n",
      "\u001B[1;31mValueError\u001B[0m: Optimizer (<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001CB5731E3B0>) passed to `model.compile` was created inside a different distribution strategy scope than the model. All optimizers must be created in the same distribution strategy scope as the model (in this case <tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x000001CB5731F250>). If you pass in a string identifier for an optimizer to compile, the optimizer will automatically be created in the correct distribution strategy scope."
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plotting the loss values for all iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot loss values for each iteration\n",
    "for i, loss_values in enumerate(all_losses):\n",
    "    plt.plot(loss_values, label=f'Iteration {i+1}')\n",
    "\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found:\")\n",
    "print(best_pars)"
   ],
   "id": "419409a69bf141c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
